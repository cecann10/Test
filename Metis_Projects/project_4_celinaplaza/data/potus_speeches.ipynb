{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Get the data required for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist, pull in csv file received from [Kaggle](https://www.kaggle.com/littleotter/united-states-presidential-speeches) that includes presidential speeches from every US President starting with Washingon on 1789-04-30 to Trump on 2019-09-25.  Each row includes:\n",
    "1. Date of speech\n",
    "2. President\n",
    "3. Party of President\n",
    "4. Speech Title\n",
    "5. Summary of Speech\n",
    "6. Transcript\n",
    "7. URL of source of transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in full file of presidential speeches\n",
    "\n",
    "potus_speech = pd.read_csv('presidential_speeches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Exploratory Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many speeches are within this file\n",
    "\n",
    "len(potus_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discovered one speech that is missing, so will remove from data\n",
    "\n",
    "potus_speech.dropna(subset=['Transcript'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify column deleted\n",
    "\n",
    "len(potus_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'President', 'Party', 'Speech Title', 'Summary', 'Transcript',\n",
       "       'URL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potus_speech.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = potus_speech['Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7432\n"
     ]
    }
   ],
   "source": [
    "for document in transcripts:\n",
    "    tokens = TreebankWordTokenizer().tokenize(document)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_pipe:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 cleaning_function, \n",
    "                 vectorizer=CountVectorizer(), \n",
    "                 tokenizer=TreebankWordTokenizer().tokenize, \n",
    "                 stemmer=PorterStemmer()):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self._is_fit = False\n",
    "    \n",
    "    def fit(self, text):\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._is_fit = True\n",
    "    \n",
    "    def transform(self, text):\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        \n",
    "        return self.vectorizer.transform(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_pipe_v2:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 cleaning_function, \n",
    "                 vectorizer=CountVectorizer(), \n",
    "                 tokenizer=TreebankWordTokenizer().tokenize, \n",
    "                 stemmer=PorterStemmer()):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self._is_fit = False\n",
    "    \n",
    "    def fit(self, text):\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._is_fit = True\n",
    "    \n",
    "    def transform(self, text):\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        vectorized = self.vectorizer.transform(clean_text)\n",
    "        \n",
    "        return pd.DataFrame(vectorized.toarray(),\n",
    "                           columns = self.vectorizer.get_feature_names()\n",
    "                           ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brendan version\n",
    "\n",
    "def cleaning_function_v2(text, tokenizer, stemmer):\n",
    "    clean_text = []\n",
    "    for speech in text:\n",
    "        tokens = tokenizer(speech)\n",
    "        \n",
    "        stemmed = []\n",
    "        for token in tokens:\n",
    "            stemmed.append(stemmer.stem(token))\n",
    "            \n",
    "        clean_document = \" \".join(stemmed)\n",
    "        clean_text.append(clean_document)\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leon version with slight edit to stem no matter what\n",
    "\n",
    "def cleaning_function(text, tokenizer, stemmer):\n",
    "    cleaned_text = []\n",
    "    for words in text:\n",
    "        cleaned_words = []\n",
    "        for word in tokenizer(words):\n",
    "            low_word = stemmer.stem(word.lower())\n",
    "            cleaned_words.append(low_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nlp_pipe(cleaning_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_v2 = nlp_pipe_v2(cleaning_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.fit(transcripts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_v2.fit(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_transformed = nlp_v2.transform(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117\n",
       "1     25\n",
       "2     76\n",
       "3    120\n",
       "4    117\n",
       "Name: the, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_transformed['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991, 32122)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#old method that at least gets array for nlp (version 1)\n",
    "\n",
    "transcripts_vec = nlp.transform(transcripts).toarray()\n",
    "\n",
    "transcripts_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
